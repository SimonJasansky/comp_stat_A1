---
title: "Comp Stat - Assignment 2"
author: "Akos Engelmann, Gergely Paradi, Fabian Gallyas, Ipek Cakin, Simon Jasansky"
date: "2023-03-21"
output: 
  pdf_document:
    number_sections: true
bibliography: references.bib 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("ggplot2")
library("bootstrap")
library("magrittr")
```


```{r, include=FALSE}
# visualize the data
data(law)
law %<>% add_column(observation = 1:nrow(law), .before = 1)
ggplot(law, aes(x = LSAT, y = GPA)) +
geom_text(aes(label = observation),
hjust = 0, vjust = 0)
```



# Task 0: Do the vanilla bootstrap for the distribution of the pearson correlation coefficient 
```{r, include=FALSE}

# calculate the correlation between the data for the original dataframe
cor(law$GPA, law$LSAT)

# function takes in a dataframe (in our case the law dataframe)
boot <- function(x) {
  # generate a list of 15 integers, that are between 1 and 15 (randomly)
  ids <- sample(nrow(law), replace = TRUE)
  # calculate the correlation when selecting the rows as given in ids
  cor <- cor(law[ids,]$GPA, law[ids,]$LSAT)
  #return the correlation coefficient
  cor
}

# apply it only once on the original dataframe
print(boot(law))

cor_dist_boot <- replicate(40000, boot(law$LSAT))
hist(cor_dist_boot)

```


# Task 1: Do the bootstrap with exhaustive enumeration 

Get get a dataframe with all the possible combinations
Code inspired by: https://christofseiler.github.io/stats205/Lecture5/BootstrapPart2.html#1 
```{r, include=FALSE}

# load law data again
data(law)
# install.packages("partitions")
library("partitions")

# define the length of our sequence
n = 15
comps = partitions::compositions(n,n)

# it returns all possible compositions in the columns
comps[,1:5]

# check how many compositions there are 
comp_len = dim(comps)[2]

# check if true
dim(comps)[2] == choose(2*n-1,n-1)
dim(comps)[2] == factorial(29)/(factorial(15)*factorial(14))

```


Get an understanding how we can build a function by running the most simple example by taking the third column of the comps dataframe 
```{r, include=FALSE}

# define the third column of the composition as test
indices = comps[,2]

# define the function that is used inside the bootstrap function
# it creates a list of length 15, where the nth element contains i row of the original data x times, as specified in comps
j = 1
create_law_list = function(j) {
  matrix(rep(law[j,], indices[j]), ncol = 2, byrow = TRUE)
}

create_law_list(3)
test_list = lapply(1:n, create_law_list)

# so the inner_function returns a list of matrices
# each element in the list, i.e. each matrix, contains as many observations of row 1-15 as specified in comps

# collapse the list into one matrix with as many observations of each row as specified in comps
l_list_new_test = do.call(rbind, test_list)
print(l_list_new_test)

# calculate correlation between the columns
correlation = cor(unlist(l_list_new_test[,1]), unlist(l_list_new_test[,2]))

# calculate the weight for each correlation, which corresponds to the probability of observing this exact set by random chance. 
# This is comparable to a roll of two fair dice, where rolling the sum of 3 is more likely (2/36) than the sum of 2 (1/36)
# see https://statweb.stanford.edu/~cgates/PERSI/papers/graycodes.pdf section 3 for why we have to append the probability/weight 
dmultinom(indices, prob = rep(1,15))

```


Do the actual implementation of the exhaustive enumeration without parallelizing
```{r, include=FALSE}
### define function for bootstrap ### 

# i here is each column of the composition matrix
bootstrap = function(i) {
  
  setTxtProgressBar(pb, i)
  
  # access the ith column of the composition matrix
  indices = comps[,i]

  # apply the numbers 1 to 15 as the j input to the inner function
  # this returns a list of matrices with as many obs. as specified in comps
  law_list = lapply(1:15, create_law_list)
  
  # this collapses the list into one matrix
  law_matrix = do.call(rbind, law_list)
  
  # calculate the correlation between the two columns
  correlation = cor(unlist(law_matrix[,1]), unlist(law_matrix[,2]))
  
  # output the correlation and the weight
  # the weight is calculated by the multinomial probability of observing this set of the index
  c(correlation, dmultinom(indices, prob = rep(1,15)))
}
```


# Run the bootstrap if the file does not yet exist
```{r, include=FALSE}

# define on how many of the combinations you want to run it
# if you run it on the total amount of combinations, it will take very long!
# if fraction_of_len = 0.01, it will run on 1% of the data only. 
fraction_of_len = 0.05
n_combinations = round(comp_len*fraction_of_len)
print(n_combinations)

# Initializes the progress bar
pb <- txtProgressBar(min = 0,      # Minimum value of the progress bar
                     max = n_combinations, # Maximum value of the progress bar
                     style = 3,    # Progress bar style (also available style = 1 and style = 2)
                     width = 50,   # Progress bar width. Defaults to getOption("width")
                     char = "=")   # Character used to create the bar


# track present time 
start_time = proc.time()

# run the boostrap
output = lapply(1:n_combinations, bootstrap)

# track end time
end_time = proc.time()

time = end_time - start_time
time = time[3]

est_time_sec = time * (1/fraction_of_len)
est_time_h = est_time_sec/(60*60)

print(c("Estimated hours to run the whole sample: ", toString(est_time_h)))

output = t(simplify2array(output))
colnames(output) = c("cor","weight")
save(output, file = "law_enum_corr.Rdata")
hist(output)

```



```{r, include=FALSE}

# Does not work yet!!
# This code will probably only work on windows!

library(parallel)
# install.packages("doParallel")
# install.packages("snow")
# library("doParallel")
library("snow")


no_cores <- detectCores(logical = TRUE)  # returns the number of available hardware threads, and if it is FALSE, returns the number of physical cores

# allocate this number of available cores to the R and provide a number of clusters and then register those clusters. 
# If you specify all the cores to the R, you may have trouble doing anything else on your machine, so it is better not to use all the resources in R.
cl <- makeCluster(no_cores-1)  


if(!file.exists("enumData.Rdata")) {
  
  ptm = proc.time()
  
  # Initializes the progress bar
  pb <- txtProgressBar(min = 0, max = comp_len, style = 3, width = 50, char = "=")
  
  # track present time 
  start_time = proc.time()
  
  # run the boostrap
  output = clusterApply(cl, 1:comp_len, bootstrap)

  end_time = proc.time()

  enumData = t(simplify2array(enumData))
  colnames(enumData) = c("cor","weight")
  save(enumData,file = "enumData.Rdata") 
  
  # track end time
  time = end_time - start_time
  time = time[3]
  est_time_h = est_time_sec/(60*60)
  
  print(c("Estimated hours to run the whole sample: ", toString(est_time_h)))
 
} else {
  load("enumData.Rdata")
}

stopCluster(cl)

```


Run the exhaustive enumeration with the help of grey codes
```{r, include=FALSE}

# check if the compositions only differ in one place
comps[,1:10]

sum(abs(comps[,1] - comps[,2]))

sum_abs = c()
for(i in c(1:100)){
  sum_abs = c(sum_abs, sum(abs(comps[,i] - comps[,i+1])))
}
sum_abs
# they dont differ in only one place, so we cant use it

```


Cleaned code for creating gray codes for combinations
Adapted from https://statweb.stanford.edu/~cgates/PERSI/papers/graycodes.pdf 
```{r, include=FALSE}

# gray is the function that will be called multiple times
# x is the current pi
# point is the (d,i,p) vector of three pointers
gray <- function(x = 0, n = sum(x), 
                 k = len(x), point = c(0, 0, 0)) {
  
  # if x is a list, extract the point 
  # probably can be deleted
  if (is.list(x)) {
    point <- x$point
    x <- x$x
  }
  
  # set the final variable to false
  final <- FALSE
  
# 1. Tour
  if (sum(x) == 0) {
    x <- c(n, rep(0, k - 1))
    point <- c(1, 2, 1)
    return(list(x = x, point = point, final = final))
  }
  
# finished
  if (x[k] == n) {
    stop("alreadylast")
  }
  
  if (point[3] == 1) {
    b <- which(x != 0)[2] #might not work
    
    if (is.na(b)) b <- 2
    
    if (b == 2) {
      if((point[1] == 1) && (x[1] == 1)){
        point <- c(1,2,2)
      }
    }
    
    # dont forget i 
    else {
      if (((n - x[1]) %% 2) == 0) {
        point <- c(1, 2 ,2)
      } else if ((x[b] %% 2) == 1) {
        point <- c(1, b, b)
      } else {
        point[1:2] <- c(b, 1)
      }
    }
  # end of case b==2
  }
  else {
    # end of case p==1
    if (((n - x[point[3]]) %% 2) == 1) {
      point[1:2] <- c(point[3], point[3] - 1)
      if ((x[point[3]] %% 2) == 0) {
        point[2] <- 1
        point[3] <- point[2]
      }
    }
    else {
      if (((x[point[3] + 1]) %% 2) == 0) {
        point[1:2] <- c(point[3], point[3] + 1)
        if (x[point[3]] == 1) {point[3] <- point[3] + 1}
      }
      else {
        point[1:2] <- c(point[3] + 1, point[3])
      }
    }
  }
  x[point[2]] <- x[point[2]]+1
  x[point[1]] <- x[point[1]]-1
  if(x[1]>0) point[3]<-1
  if(x[k] == n) final <- TRUE
  return(list(x = x, point = point, final = final))
  
}



callgray <- function(n = 5, k = 5, point = c(0, 0, 0), x = 0, pasapas = FALSE) {
  
  rows <- choose(2*n-1,n-1)
  
    # Initializes the progress bar
  pb <- txtProgressBar(min = 0,      # Minimum value of the progress bar
                       max = rows, # Maximum value of the progress bar
                       style = 3,    # Progress bar style (also available style = 1 and style = 2)
                       width = 50,   # Progress bar width. Defaults to getOption("width")
                       char = "=")   # Character used to create the bar

  
  bi <- matrix(0, nrow = rows, ncol = n)
  final <- FALSE
  i <- 1
  while (!final) {
    setTxtProgressBar(pb, i)
    #print(round(i/rows*100))
    x <- gray(x, n, k, point)
    bi[i, ] <- x$x
    # print(bi)
    i <- i + 1
    final <- x$final
    if(pasapas) {
      rep <- scan(n=1)
      if(length(rep)>0)
        break
    }
  }
  print("finished")
  return(bi)
}
```


Run the algorithm and create the grey codes for combinations
```{r, include=FALSE}
if(!file.exists("graycodes15.Rdata")) {
graycodes_15 = callgray(n = 15, k = 15, point = c(0, 0, 0), x = 0)
save(graycodes_15, file = "graycodes15.Rdata")
} else {
  load("graycodes15.Rdata")
}
```


Run the correlation with the gray code combination
This should be faster

Define the function
```{r, include=FALSE}
#exact formulation is from : https://stats.stackexchange.com/questions/410468/online-update-of-pearson-coefficient

n = 15

# create first sample
create_sample = function(j) {
  matrix(rep(law[j,], indices[j]), ncol = 2, byrow = TRUE)
}

indices = graycodes_15[1,] 
test_list = lapply(1:15, create_sample)
sample = do.call(rbind, test_list)
sample = data.frame(matrix(c(unlist(sample[,1]), 
                             unlist(sample[,2])), 
                           ncol = 2, byrow = F))

# initialize all the values
x = sum(sample$X1)
y = sum(sample$X2)
a = sum(sample$X1**2)
b = sum(sample$X2**2)
c = sum(sample$X1*sample$X2)
  
correlation = (c - ((x*y)/n)) / ( sqrt(a - ((x**2)/n)) * sqrt(b - (y**2/n)))

print(c(x,y,a,b,c))
print(correlation)


# define on how many of the combinations you want to run it
# if you run it on the total amount of combinations, it will take very long!
# if fraction_of_len = 0.01, it will run on 1% of the data only. 
fraction_of_len = 0.05
n_combinations = round(dim(graycodes_15)[1]*fraction_of_len)
print(n_combinations)

# if you want you can set the number of combinations manually
# n_combinations = 5

# Initializes the progress bar
pb <- txtProgressBar(min = 2,      # Minimum value of the progress bar
                     max = n_combinations, # Maximum value of the progress bar
                     style = 3,    # Progress bar style (also available style = 1 and style = 2)
                     width = 50,   # Progress bar width. Defaults to getOption("width")
                     char = "=")   # Character used to create the bar



# loop over all the rows
# start from 2 as we already have used the first sample above
gray_enum <- function(i){
  
  # progress bar
  setTxtProgressBar(pb, i)
  
  diff = graycodes_15[i,] - graycodes_15[i-1,]

  # get indices where the observations will be exchanged
  idx_out = which(diff == -1)
  idx_in = which(diff == 1)

  x_out = law[idx_out,1]
  y_out = law[idx_out,2]
  x_in = law[idx_in,1]
  y_in = law[idx_in,2]
  
  # print(c(x_out, y_out, x_in, y_in))
  # print(c(x,y,a,b,c))
  
  x_up <- x - x_out + x_in
  y_up <- y - y_out + y_in
  a_up <- a - x_out**2 + x_in**2
  b_up <- b - y_out**2 + y_in**2
  c_up <- c - x_out*y_out + x_in*y_in
  # print(c(x,y,a,b,c))

  updated_correlation = (c_up - ((x_up*y_up)/n)) / (sqrt(a_up - ((x_up**2)/n)) * sqrt(b_up - (y_up**2/n)))
  # print(updated_correlation)
  
  
  # update global variables
  assign("x", x_up, envir = .GlobalEnv)
  assign("y", y_up, envir = .GlobalEnv)
  assign("a", a_up, envir = .GlobalEnv)
  assign("b", b_up, envir = .GlobalEnv)
  assign("c", c_up, envir = .GlobalEnv)
  
  c(updated_correlation, dmultinom(graycodes_15[i,], prob = rep(1,15)))
  
}
```


Run the function defined above if there is no output file yet
```{r, include=FALSE}

# track present time 
start_time = proc.time()

# run it
output = lapply(2:n_combinations, gray_enum)

# track end time
end_time = proc.time()

time = end_time - start_time
time = time[3]

est_time_sec = time * (1/fraction_of_len)
est_time_h = est_time_sec/(60*60)

print(c("Estimated hours to run the whole sample: ", toString(est_time_h)))


output = t(simplify2array(output))
colnames(output) = c("cor","weight")
save(output, file = "law_grey_corr.Rdata")
```


